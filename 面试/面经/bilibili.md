# bilibili面经

## 1.stlmap容器中，遍历将一个元素删除，怎么保证后序迭代器有效


### 使用 erase() 的返回值 (C++11 及以上版本)

std::map::erase() **函数在接收一个迭代器作为参数时，会返回一个指向被删除元素下一个元素的有效迭代器。[**

**工作原理：**
通过将 **erase()** **的返回值重新赋给当前的迭代器，可以直接将迭代器安全地更新到下一个有效位置，从而继续遍历。**

```
#include <iostream>
#include <map>

int main() {
    std::map<int, std::string> myMap = {{1, "A"}, {2, "B"}, {3, "C"}, {4, "D"}};

    // 遍历并删除所有 key 为偶数的元素
    for (auto it = myMap.begin(); it != myMap.end(); /* 循环体内手动控制迭代器前进 */) {
        if (it->first % 2 == 0) {
            // erase返回下一个有效迭代器
            it = myMap.erase(it); 
        } else {
            ++it; // 对于不删除的元素，正常递增
        }
    }

    for (const auto& pair : myMap) {
        std::cout << pair.first << ": " << pair.second << std::endl;
    }

    return 0;
}
```

### 方法二：使用迭代器的后置自增 (适用于所有 C++ 版本)

在 C++11 之前的版本中，erase() **函数返回** **void**在这种情况下，可以利用后置自增（**it++**）的特性来安全地删除元素

**工作原理：**
表达式 **myMap.erase(it++)** **会先对** **it** **进行一次拷贝，然后** **it** **本身自增到下一个元素，最后将** **it** **的拷贝（即指向当前待删除元素的迭代器）传递给** **erase()** **函数。这样，在删除当前元素之前，迭代器** **it** **已经安全地指向了下一个元素。**

**code**C++

```
#include <iostream>
#include <map>

int main() {
    std::map<int, std::string> myMap = {{1, "A"}, {2, "B"}, {3, "C"}, {4, "D"}};

    // 遍历并删除所有 key 为偶数的元素
    for (auto it = myMap.begin(); it != myMap.end(); /* 循环体内手动控制迭代器前进 */) {
        if (it->first % 2 == 0) {
            // 在删除前，it已经指向下一个元素
            myMap.erase(it++); 
        } else {
            ++it;
        }
    }

    for (const auto& pair : myMap) {
        std::cout << pair.first << ": " << pair.second << std::endl;
    }

    return 0;
}
```

### std::map **的迭代器失效规则**

**需要注意的是，**std::map**（以及** **set**、**list** **等基于节点的容器）的迭代器失效规则相对宽松：**

**删除操作 (**erase**) 只会使指向被删除元素的迭代器失效**

**指向容器中其他元素的迭代器和引用均保持有效**

2.vector扩容逻辑

3.vector map 迭代底层逻辑

4.虚函数用的场合

5.C语言实现多态应该怎么写

**结构体（**struct**）、函数指针（**function pointer**）和 **void*** **指针来模拟出多态的行为**** **。核心思想是**手动创建一个类似于 C++ 中的“虚函数表”（vtable）的结构。

6.析构函数为什么是虚函数

7.构造函数什么时候定义为私有

8.lamdba 如果值捕获和引用捕获哪种会让计数加一

9.weak作用

10.多线程 自旋锁和互斥锁区别

11.conditon_variable wait的时候unique_lock是放掉还是持有

12.notify时候需要在已经有锁的环境下

13.右值引用

14.内存对齐

15.动态链接和静态链接区别   

16.排序的稳定性指什么

**递归时没有写退出条件会导致程序报错，原理是什么？**
原理是**栈溢出（Stack Overflow）**
每次函数调用（包括递归调用）时，系统都会在程序的调用栈（Call Stack）上分配一个栈帧（Stack Frame），用来存储函数的局部变量、参数和返回地址。如果递归没有退出条件，它会无限地调用自身，导致调用栈上不断地累积新的栈帧。[[18](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQFPq8nt2hMWRD9Neas07jOnJAq1FXU8APmxs4DbixkfOjEsDASVbYU2uiU-KAIjHEEw63M75DgGg1cynlrOfjh63sbSW8A1LwTe-E2eoSUcpS9JtSxMULu6oJKV-svqUfgOD4CMVyDzeCdBKkxTgN1VpgHqGifLCKtQ)]
由于栈的内存空间是有限的，当栈帧的数量超出了栈的容量时，就会发生栈溢出，程序会因此崩溃并通常会报告一个段错误（Segmentation Fault）。

**同一个进程的不同线程之间，栈是共享的吗？**
不共享
在同一个进程中，不同的线程拥有各自独立的栈空间。每个线程的栈用来存储该线程自己的函数调用、局部变量等信息，这样可以保证线程的执行是相对独立的，一个线程的函数调用不会干扰到另一个线程。
但是，它们共享进程的**堆内存（Heap）**、**全局变量**、**静态变量**和**代码段**等资源。这也是为什么在多线程环境下访问这些共享资源时需要使用锁等同步机制来避免数据竞争。

### 互斥锁和自旋锁的区别，使用场景分别是什么

**互斥锁（Mutex）和自旋锁（Spinlock）是多线程编程中用于保护共享资源的两种常见同步机制，它们的核心区别在于线程等待锁的方式不同。推荐mutex**

 **区别**:

    互斥锁 内核阻塞休眠  上下文切换两次，睡眠唤醒

   自旋锁  不断获取，忙等待，用户态，cpu空转时间

* **等待方式**：当一个线程尝试获取一个已经被占用的互斥锁时，该线程会进入睡眠状态，CPU会切换到其他可执行的线程。[[1](https://www.google.com/url?sa=E&q=https%3A%2F%2Fvertexaisearch.cloud.google.com%2Fgrounding-api-redirect%2FAUZIYQF2bPXwmvdh2iG9AAk1ghIOnIho_OU_O-aKSoysHMrrNT1qvOAXiYP4RQE172jO6F06hVkL3NDj-5s0ZVW52wy1Ue5YlnWRTfEVIflDgm1o5S6MzbKvxxjHmoHbx0xArxY7JqD5hjaXRPI0lWHyemKRR3O6)] 而当线程尝试获取一个已经被占用的自旋锁时，它不会放弃CPU，而是会进入一个忙等待（busy-waiting）的循环，反复检查锁是否已经释放。
* **开销**：互斥锁的开销主要在于线程的上下文切换和休眠唤醒。如果锁被占用的时间很长，这种方式可以节省CPU资源。自旋锁的开销在于“自旋”等待期间会持续消耗CPU时间。
* **实现层面**：自旋锁更为底层，通常由CPU提供的原子操作（如CAS）实现，而互斥锁的实现可能依赖于自旋锁。

 **使用场景**:

* **自旋锁**：适用于锁的持有时间非常短的场景。如果临界区的代码执行速度很快，线程自旋等待的CPU开销会小于线程上下文切换的开销。自旋锁也适用于多核处理器环境，因为等待的线程可以在另一个核心上运行，而不会影响持有锁的线程。
* **互斥锁**：适用于锁的持有时间较长，或者在临界区内有I/O操作等可能导致线程长时间等待的场景。 在这种情况下，让等待的线程休眠可以避免浪费CPU资源。

共享内存的用法以及使用场景

#### **参考答案**

内存共享允许多个进程直接读写同一块物理内存；它避免内核数据拷贝，是最高效的进程间通信方式；主要通过 `mmap` 和 `munmap` 实现。

#### **共享内存的三种常用实现方式**

##### **1. 基于内存文件系统 tmpfs，适用于非亲缘进程间高速通信**

* **步骤** ：

1. `shm_open` 创建共享内存
2. `ftruncate` 设置大小
3. `mmap` 创建映射
4. 直接使用共享内存
5. 使用后清理：`munmap` → `close` → `shm_unlink`

##### **2. 基于普通文件，适用于需持久化（重启生效）的进程间数据共享（如配置文件、大数据缓存）**

* **步骤** ：

1. `open` 创建或打开文件
2. `ftruncate` 设置大小
3. `mmap` 创建映射
4. 直接使用共享内存
5. 可通过 `msync` 同步数据到磁盘
6. 清理：`munmap` → `close` → `unlink`

##### **3. 匿名映射，适用于亲缘进程（父子/兄弟）间高效共享**

* **步骤** ：

1. 通过 `mmap` 创建映射
2. 父子进程通过 `fork` 继承映射区域
3. 直接使用共享内存

### **编译、链接与加载**

#### 1.  请简单描述一下一个 .c 或 .cpp 文件从源码到可执行文件会经历哪些步骤？

**一个 C/C++ 源文件到可执行文件的过程主要分为四个阶段：**

* **预处理 (Preprocessing)**：此阶段处理源代码中以 **#** **开头的指令。主要工作包括：展开头文件（如** **#include**）、替换宏定义（**#define**）、处理条件编译指令（**#if**, **#ifdef**, **#endif** **等）以及删除注释。 这个过程的输出是一个扩展后的** **.i**（C）或 **.ii**（C++）文件。
* **编译 (Compilation)**：编译器（如 GCC、Clang）将预处理后的文件转换成汇编代码。 这个阶段会进行词法分析、语法分析、语义分析和优化，最终生成一个 **.s** **文件。**
* **汇编 (Assembly)**：汇编器（如 **as**）将汇编代码转换成机器可以执行的二进制指令，即目标文件（Object File）。这个文件的格式通常是 ELF（Executable and Linkable Format），后缀为 **.o**。
* **链接 (Linking)**：链接器（如 **ld**）将一个或多个目标文件以及它们所需的库文件链接在一起，生成最终的可执行文件。链接过程主要解决符号引用和地址重定位的问题。

#### 2. 🔗静态链接和动态链接有什么区别？

**静态链接和动态链接是两种不同的库文件链接方式，主要区别在于链接发生的时间和库代码被整合到可执行文件的方式。**

| **特性**           | **静态链接**                                                                         | **动态链接**                                                                                 |
| ------------------ | ------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------- |
| **链接时间**       | **在编译链接时完成。**                                                               | **在程序启动或运行时完成。**                                                                 |
| **库代码包含方式** | **库的代码被完整地复制到最终的可执行文件中。**                                       | **可执行文件中只包含对库的引用，库代码本身不被复制。**                                       |
| **文件大小**       | **生成的可执行文件体积较大，因为它包含了所有依赖库的代码。**                         | **生成的可执行文件体积较小。**                                                               |
| **内存占用**       | **如果多个程序都静态链接了同一个库，那么这个库的副本会在内存中存在多份，造成浪费。** | **多个程序可以共享内存中同一份动态库的副本，节省内存。**                                     |
| **更新与维护**     | **如果库文件有更新，所有依赖它的程序都必须重新链接和编译。**                         | **只需要更新动态库文件本身，所有依赖它的程序在下次运行时就会自动使用新版本，无需重新编译。** |
| **运行时依赖**     | **没有外部依赖，可以在任何没有安装相应库的环境中运行。**                             | **必须依赖目标系统中存在相应版本的动态库文件才能运行。**                                     |

#### 3. 链接阶段具体会做哪些工作？

**链接阶段的主要任务是将多个独立编译的目标文件（**.o **文件）和库文件组合成一个单一的可执行文件。其核心工作有两个：**

* **符号解析 (Symbol Resolution)**：编译器在生成目标文件时，会把源文件中引用的函数和全局变量标记为符号。链接器的任务是找到每个符号的定义。它会扫描所有的输入目标文件和库文件，为每个引用的符号找到其唯一的定义位置。如果找不到或者找到多个定义，链接器就会报错。
* **重定位 (Relocation)**：一旦符号解析完成，链接器就知道了所有代码和数据段的确切大小。它会合并所有目标文件的同类型段（如 **.text** **段合并、**.data **段合并），并为它们分配运行时的虚拟地址。然后，链接器会修改代码中对符号的引用，将这些符号的占位地址替换成它们最终的虚拟地址。**

#### 4. 你是否使用过 dlopen 这类 API？如果用过，简单讲讲如何使用。

**是的，我使用过。**dlopen **系列 API 用于在程序运行时动态地加载和使用共享库（**.so **文件），也称为插件化或动态加载。**

**基本使用流程如下：**

* **void* dlopen(const char* filename, int flag)**：加载一个共享库。

  * **filename**：共享库的路径。
* **flag**：加载模式，常用的是 **RTLD_LAZY**（延迟符号解析，用到时才解析）或 **RTLD_NOW**（立即解析所有符号）。
* **返回值**：成功时返回一个库句柄，失败时返回 **NULL**。
* **void* dlsym(void* handle, const char* symbol)**：在已加载的库中查找一个符号（函数或变量）的地址。

  * **handle**：**dlopen** **返回的库句柄。**
* **symbol**：要查找的符号名称（字符串）。
* **返回值**：成功时返回符号的地址，失败时返回 **NULL**。
* **int dlclose(void* handle)**：卸载一个已加载的库。当库的引用计数降为零时，它会被真正从内存中移除。

  * **返回值**：成功时返回 0，失败时返回非零值。
* **char* dlerror(void)**：用于获取上述函数调用失败时的错误信息。

#### 5.  当你更新一个被 dlopen 加载的 .so 文件时，整个服务需要重启吗？需要注意什么 ABI 兼容问题？

 **不一定需要重启**。可以通过调用 **dlclose()** **卸载旧版本的** **.so**，然后再调用 **dlopen()** **加载新版本的文件，从而实现在线热更新。**

  **需要注意的问题**：

* **状态保存**：在卸载旧库之前，需要确保所有指向旧库内存（包括函数指针和对象实例）的引用都已释放。如果库中有全局状态或单例，需要设计一种机制来迁移或重新初始化这些状态。
* **ABI 兼容性**：ABI（Application Binary Interface，应用二进制接口）是程序在机器级别上交互的规范。更新 **.so** **文件时必须高度关注 ABI 兼容性，否则会导致程序崩溃或未定义行为。**

  * **ABI 兼容的修改**：
  * **新增非虚函数。**
  * **在类末尾添加新的非虚成员变量。**
  * **新增静态成员变量。**
* **ABI 不兼容的修改（破坏性变更）**：

  * **改变现有函数签名（参数类型、数量、返回值）。**
    * **改变虚函数的顺序或删除虚函数。**
    * **改变类成员变量的顺序、类型或删除成员变量。**
    * **改变类继承体系。**

**如果发生了 ABI 不兼容的变更，强行热更新几乎一定会导致严重问题。安全的做法是，只有在确认 ABI 兼容的情况下才进行热更新，否则必须重启整个服务。**

#### 6.  C++ 中 extern "C" 的作用是什么？

 **extern "C"** **的主要作用是**指示编译器以 C 语言的方式来编译和链接指定的代码**。**

  **C++ 支持函数重载，为了实现这一点，C++ 编译器会对函数名进行**名字修饰**，将函数名、参数类型等信息编码成一个在链接时唯一的符号名。而 C 语言不支持函数重载，其函数名在编译后通常保持不变。**

 **extern "C"** **的作用就是告诉 C++ 编译器：**

* **对于函数声明**：这个函数是按照 C 语言的规则来链接的，不要对它进行名字修饰，直接使用原始的函数名作为符号。
* **对于函数定义**：以 C 语言的方式编译这个函数，使其能被 C 代码或其他 **extern "C"** **声明的代码调用。**

**这使得 C++ 代码能够安全地调用 C 语言编写的库函数，同时也允许 C 代码调用 C++ 编写的函数。**

### 进程、线程与同步

#### 7. 进程和线程有什么区别？

**进程（Process）和线程（Thread）是操作系统进行任务调度和资源分配的基本单位，它们的主要区别如下：**

* **资源所有权**：**进程是资源分配的基本单位**。每个进程都有自己独立的地址空间、内存、文件句柄等系统资源。一个进程崩溃通常不会影响到其他进程。 **线程是 CPU 调度的基本单位**。线程自己不拥有系统资源，它与同属一个进程的其他线程共享该进程的全部资源。
* **地址空间**：不同进程的地址空间是相互隔离的。而同一进程内的所有线程共享同一个地址空间。
* **切换开销**：进程切换时，需要切换整个页表和内核上下文，开销较大。线程切换仅需保存和恢复少量寄存器和栈信息，开销远小于进程切换。
* **通信**：进程间通信（IPC）需要通过操作系统提供的特定机制（如管道、共享内存、套接字等）。线程间通信则非常方便，因为它们可以直接读写共享的全局变量和堆内存。
* **创建与销毁**：创建和销毁进程的开销比线程大得多。

#### 8. 线程间如何保证同步与互斥？请举出常用机制。

**线程同步与互斥是为了解决在多线程环境下访问共享资源时可能出现的数据竞争和不一致问题。常用机制包括：**

* **互斥锁 (Mutex)**：保证在任何时刻，只有一个线程可以访问被保护的代码临界区或数据。线程在进入临界区前获取锁，离开时释放锁。如果锁已被占用，尝试获取锁的线程会被阻塞。
* **条件变量 (Condition Variable)**：通常与互斥锁配合使用，允许线程在某个条件不满足时挂起等待，直到其他线程改变了该条件并通知它。 它解决了单纯使用互斥锁进行轮询等待（busy-waiting）的低效问题。
* **信号量 (Semaphore)**：一个计数器，用于控制同时访问某个特定资源的线程数量。它允许多个线程同时进入临界区（只要计数值大于零）。
* **读写锁 (Read-Write Lock)**：一种更细粒度的锁，允许多个线程同时进行读操作，但只允许一个线程进行写操作。适用于“读多写少”的场景，可以提高并发性。
* **原子操作 (Atomic Operations)**：由 CPU 硬件保证其执行是不可分割的操作，如 **std::atomic** **提供的类型。对于简单的计数器增减、标志位修改等操作，使用原子操作比使用锁的开销更小。**

#### 9. 进程间通信（IPC）有哪些常见方式？

**进程间通信（IPC）是指不同进程之间交换数据和信息的方式。常见方式有：**

* **管道 (Pipe)**：半双工通信，数据只能单向流动，且通常用于有亲缘关系的进程（如父子进程）之间。
* **命名管道 (FIFO)**：也叫有名管道，它在文件系统中有一个路径名。允许无亲缘关系的进程之间进行通信。
* **共享内存 (Shared Memory)**：将一块物理内存映射到多个进程的虚拟地址空间中，使得它们可以直接访问这块内存。这是最快的 IPC 方式，但需要用户自己处理同步问题。
* **消息队列 (Message Queue)**：一个由内核维护的消息链表。进程可以向队列中发送和接收消息，克服了管道和 FIFO 只能传输无格式字节流的缺点。
* **信号量 (Semaphore)**：主要用作同步或互斥，控制对共享资源的访问，也可以作为一种简单的通信方式。
* **信号 (Signal)**：用于通知接收进程某个异步事件已经发生。
* **套接字 (Socket)**：最通用的 IPC 方式，不仅可以用于同一台机器上的进程间通信，还可以用于不同机器间的网络通信。

#### 10. 使用 pipe 进行通信时需要考虑同步互斥吗？共享内存呢？

* **管道 (Pipe)**：**不需要**用户自己考虑同步互斥。管道本身是由内核管理的，内核保证了对管道读写的原子性。当管道为空时，读操作会阻塞；当管道为满时，写操作会阻塞。操作系统已经内置了同步机制。
* **共享内存 (Shared Memory)**：**必须**由用户自己保证同步互斥。操作系统只负责分配一块内存区域并将其映射到多个进程的地址空间，但它不提供任何内置的同步机制。多个进程对共享内存的并发读写会立刻导致数据竞争。因此，使用共享内存时必须配合其他同步原语（如信号量、互斥锁等）来保证数据的一致性。

### 内存管理与操作系统

#### 11. 请描述一次 CPU 读内存的完整流程，从虚拟地址到拿到数据。

**CPU 读内存是一个多层次的复杂过程，涉及虚拟地址到物理地址的转换以及多级缓存系统：**

* **CPU 发出请求**：CPU 的执行单元需要读取一个虚拟地址（Virtual Address）上的数据，它将这个地址发送给内存管理单元（MMU）。
* **MMU 地址翻译**：

  * **TLB 查找**：MMU 首先会查询**转译后备缓冲区（TLB）**，这是一个高速缓存，用于存放最近使用过的虚拟地址到物理地址的映射关系。如果 TLB 命中（TLB Hit），MMU 直接获得物理地址，过程跳转到第 4 步。
* **TLB 未命中 (TLB Miss)**：如果 TLB 中没有对应的条目，MMU 需要通过查询**页表（Page Table）**来完成地址翻译。CPU 会根据页表基地址寄存器（如 CR3）找到页表在内存中的位置。
* **多级页表查询**：在现代系统中，通常是多级页表。MMU 会逐级查询页表，最终找到虚拟页号对应的物理页号。
* **更新 TLB**：找到映射关系后，MMU 会将其存入 TLB，以便下次快速访问。
* **缺页中断 (Page Fault)**：如果在查询页表时发现对应的页不在物理内存中（例如，它被换出到磁盘了），会触发一个**缺页异常**。操作系统内核会介入，将数据从磁盘加载到物理内存，更新页表，然后返回到原来的指令继续执行。
* **访问高速缓存 (Cache)**：拿到物理地址后，CPU 并不会直接访问主存（DRAM），而是先访问多级高速缓存系统：

  * **L1 Cache 查询**：首先查询速度最快但容量最小的 L1 Cache。如果数据在 L1 中（Cache Hit），CPU 直接从中读取数据，过程结束。
* **L2/L3 Cache 查询**：如果 L1 未命中（Cache Miss），则继续查询速度稍慢但容量更大的 L2 Cache，然后是 L3 Cache。如果命中，数据会被读取，并同时被加载到更高级别的缓存中（如 L1）。
* **访问主存 (Main Memory)**：如果所有级别的缓存都未命中，CPU 才会通过内存总线向主存发出请求，读取数据。
* **数据返回**：数据从主存中被读取出来，通过总线返回给 CPU。在返回的途中，这个数据块（一个 Cache Line 的大小）会被依次装入 L3、L2 和 L1 缓存，以便未来的快速访问。最终，CPU 获得所需的数据。

#### 12. 简述 Linux 进程的虚拟地址空间布局。

**在 32 位 Linux 系统中，一个进程拥有 4GB 的虚拟地址空间。这个空间通常被划分为用户空间和内核空间。64 位系统则拥有巨大的虚拟地址空间。**

**典型的用户空间布局从低地址到高地址依次是：**

* **只读段**：

  * **代码段 (.text)**：存放程序执行的二进制指令。
* **只读数据段 (.rodata)**：存放常量字符串等只读数据。
* **数据段**：

  * **已初始化数据段 (.data)**：存放已初始化的全局变量和静态变量。
* **未初始化数据段 (.bss)**：存放未初始化的全局变量和静态变量。内核在程序加载时会将这块区域清零。
* **堆 (Heap)**：用于动态内存分配（如 **malloc**, **new**）。堆从低地址向高地址增长。
* **内存映射区 (Memory Mapping Segment)**：用于加载动态链接库（**.so** **文件）以及通过** **mmap()** **进行文件映射或匿名映射。**
* **栈 (Stack)**：存放函数的参数、局部变量、返回地址等。栈从高地址向低地址增长。
* **内核空间 (Kernel Space)**：虚拟地址空间的高地址部分保留给内核使用，用户代码不能直接访问。当进程通过系统调用进入内核态时，CPU 才能访问这部分内存。

#### 13. 📏 一个 cache line 通常多大？

 **一个 Cache Line（缓存行）是 CPU 从主存加载数据到缓存的最小单位。它的大小是 CPU 体系结构决定的，通常是** **64 字节**。 即使你只需要读取一个字节的数据，CPU 也会将包含该字节的整个 64 字节数据块加载到缓存中。

 **这个特性是**伪共享（False Sharing）**问题的根源：如果多个线程频繁地修改位于同一个缓存行但逻辑上不相关的不同变量，会导致该缓存行在不同核心的缓存之间被频繁地无效化和同步，从而严重影响性能。**

### C++ 深度

#### 14. 🏗️C++ 的多态是如何实现的？对象模型里虚表放在哪里？

**C++ 的运行时多态是通过**虚函数（Virtual Functions）**和**虚表（Virtual Table, vtable）**机制实现的。**

 **实现原理**：

* **虚表指针 (vptr)**：如果一个类包含虚函数（或继承自包含虚函数的基类），编译器会为该类的每个对象在内存布局的起始位置（通常是）添加一个隐藏的指针，这个指针被称为**虚表指针（vptr）**。 这个 vptr 指向该类对应的虚表。
* **虚表 (vtable)**：对于每个包含虚函数的类，编译器会创建一个静态的、全局唯一的虚表。这个虚表本质上是一个函数指针数组，其中存放了该类所有虚函数的地址。 如果子类重写了基类的虚函数，虚表中对应的条目会被替换为子类重写后的函数地址。
* **动态绑定**：当通过基类指针或引用调用一个虚函数时（例如 **base_ptr->some_virtual_func()**），编译器生成的代码会执行以下操作：

  * **通过对象的 vptr 找到其对应的 vtable。**
* **在 vtable 中查找该虚函数对应的条目（偏移量在编译时是固定的）。**
* **通过函数指针调用正确的函数。**

**由于 vptr 指向的虚表取决于对象的实际类型，因此在运行时能够调用到正确版本的虚函数，从而实现了动态绑定和多态。**

 **虚表的位置**：

* **虚表指针 (vptr)**：位于**对象实例的内存空间**中，通常在最开始的位置。
* **虚表 (vtable)**：位于程序的可执行文件的**只读数据段 (.rodata)** **中。 它是类级别的，被该类的所有对象共享。**

#### 15. 虚表里大致存放什么内容？

**虚表主要存放以下内容：**

* **虚函数指针**：这是虚表最核心的内容。它是一个函数指针数组，每个元素指向一个虚函数的实现地址。其顺序在编译时确定。
* **RTTI 信息 (Run-Time Type Information)**：如果启用了 RTTI，虚表的顶部（或某个固定偏移处）通常会存放一个指向 **type_info** **对象的指针，用于支持** **dynamic_cast** **和** **typeid** **操作。**
* **虚基类偏移量**：在涉及虚拟继承的复杂场景中，虚表可能还会包含用于定位虚基类子对象的偏移量信息。

#### 16.  模板（template）在编译期起到什么作用？

 **模板是 C++ 的一种**元编程（Metaprogramming）**工具，它完全在**编译期**工作，实现了**编译时多态**。**

  **主要作用是**代码生成**：**

* **实例化 (Instantiation)**：模板本身并不是一个具体的类或函数，而是一个“蓝图”或“配方”。当编译器遇到使用模板的代码时（例如 **std::vector `<int>`** **或** **max(a, b)**），它会根据提供的模板参数（如 **int**）来**实例化**出一套具体的、针对该类型的代码。
* **静态类型检查**：所有类型相关的检查都在编译期完成。如果尝试用一个不支持模板中操作的类型来实例化模板，编译器会直接报错。
* **泛型编程**：允许我们编写与类型无关的代码，提高了代码的复用性和灵活性。

#### 17.  写代码：实现一个 shared_ptr。

 **一个** **shared_ptr** **的核心是**共享所有权的引用计数**。它通常需要两个主要部分：一个是指向托管对象的原始指针，另一个是指向一个控制块**的指针。控制块中至少包含引用计数。**

### 内存分配与性能

#### 18. 你了解 TCMalloc 和 jemalloc 吗？它们与 ptmalloc 有何差异？

**是的，我了解。TCMalloc (Thread-Caching Malloc) 和 jemalloc 都是为了解决** **glibc** **默认内存分配器** **ptmalloc** **(pthreads malloc) 在高并发多线程环境下性能瓶颈而设计的现代内存分配器。**

 **ptmalloc (glibc 默认)**：

* **设计**：为每个 CPU 核心维护一个分配区（arena），尝试通过 arena 来减少线程间的锁竞争。
* **缺点**：当线程数多于核心数时，多个线程可能会共享同一个 arena，导致访问该 arena 时仍需加锁，从而在高并发下产生严重的锁竞争。 此外，它的内存回收机制可能导致较高的内存碎片和内存占用。

 **TCMalloc (Google)**：

* **核心思想**：线程缓存（Thread Caching）。每个线程都拥有一个私有的、无锁的本地缓存（Thread-Local Cache），用于处理小对象的分配。
* **工作方式**：

  * **小对象分配**：直接从线程本地缓存中分配，完全无锁，速度极快。
* **大对象分配**：当本地缓存不足或分配大对象时，会从一个中心的、有锁的页堆（Central Page Heap）中获取内存。
* **优点**：极大地减少了多线程环境下的锁竞争，显著提升了小对象分配的性能和扩展性。

 **jemalloc (FreeBSD, Facebook)**：

* **核心思想**：与 TCMalloc 类似，也使用了线程本地缓存。但它更注重于**减少内存碎片**和**可预测的内存行为**。
* **工作方式**：它将内存块按大小（size classes）进行精细化管理，并使用多个分配区（arenas）来进一步降低锁竞争。
* **优点**：在内存利用率和碎片控制方面通常表现更好，特别是在需要长时间运行的服务中。Facebook 在其服务器上广泛使用 jemalloc。

 **主要差异总结**：

| **特性**     | **ptmalloc**                 | **TCMalloc**             | **jemalloc**                  |
| ------------ | ---------------------------- | ------------------------ | ----------------------------- |
| **核心机制** | **Per-core arenas**          | **Per-thread cache**     | **Per-thread cache & arenas** |
| **锁竞争**   | **较高（多线程共享 arena）** | **非常低（小对象无锁）** | **非常低**                    |
| **性能**     | **多线程下扩展性差**         | **小对象分配极快**       | **性能优秀，扩展性好**        |
| **内存碎片** | **较高**                     | **较低**                 | **控制得最好**                |

#### 19. 你自己实现过内存池吗？如何管理内存碎片和扩容？

**是的，在一些对性能要求极高的场景中，我实现过简单的内存池。内存池通过预先分配一大块内存，然后按需从中切分小块给用户，来避免频繁的系统调用（**malloc**/**free**），从而提高性能。**

 **管理内存碎片**：
内存碎片分为内部碎片和外部碎片。我的实现主要通过以下方式管理：

* **固定大小内存池**：这是最简单有效的方式。内存池只分配固定大小（例如 32 字节、64 字节）的内存块。所有请求都被分配一个同样大小的块，完全消除了外部碎片。内部碎片是存在的（例如请求 20 字节，但分配了 32 字节），但由于大小固定，管理非常简单。
* **分离适配 (Segregated Fit)**：创建多个不同大小规格的内存池（例如一个 16 字节池，一个 32 字节池，一个 64 字节池...）。当有内存请求时，从最合适大小的池中分配。这在固定大小的基础上提供了灵活性，同时能较好地控制碎片。
* **空闲链表 (Free List)**：将回收的内存块通过指针链接成一个链表。当有新的分配请求时，直接从链表头部取下一个块。这是一种非常高效的回收和再分配策略。

 **扩容机制**：
当内存池中所有内存块都被分配完毕后，就需要扩容。

* **分配新内存块（Chunk）**：内存池不会尝试去 **realloc** **原来的大内存块，因为这很低效。而是会向操作系统申请一个新的、同样大小（或按倍数增长）的大内存块（Chunk）。**
* **链式管理**：将新的 Chunk 加入到一个 Chunk 列表中进行管理。当分配内存时，会先在当前活动的 Chunk 中查找空闲块。如果当前 Chunk 已满，则从下一个 Chunk 中进行分配。这样，内存池就可以动态地增长。

#### 20.  volatile 关键字的作用是什么？对内存序有帮助吗？

 **volatile** **是一个类型限定符，它的核心作用是**防止编译器对变量的读写操作进行优*。

  **主要作用**：

* **抑制优化**：**volatile** **告诉编译器，这个变量的值随时可能在程序外部被改变（例如，由硬件、操作系统或其他线程）。因此，每次访问该变量时，都必须**直接从内存中重新加载**，而不是使用寄存器中的缓存值。同样，每次对它进行写操作，都必须**立即写回内存**。**
* **保证访问顺序**：在同一个线程内，**volatile** **变量的访问顺序不会被编译器重排。也就是说，代码中** **volatile** **变量的出现顺序就是它们实际执行的顺序。**

 **对内存序有帮助吗？**
非常有限**。**volatile **不能**解决多线程环境下的所有内存序问题，它**不是**一个通用的同步工具。

* **volatile** **能够防止**编译器**重排内存操作，但它**无法**阻止** **CPU** **出于性能考虑而进行的**乱序执行（Out-of-Order Execution）**。**
* **它不提供**原子性**。例如，**volatile int i; i++; **这个操作不是原子的，它仍然包含读、改、写三个步骤，在多线程下会产生竞争条件。**
* **它不提供跨线程的**可见性保证（ happens-before 关系）**。一个线程对** **volatile** **变量的写入，并不保证能被另一个线程立即看到。**

### 2. 数据库索引

#### 什么是数据库索引？

 **数据库索引是一种数据结构，它模仿了书籍末尾的索引。它的主要目的是**提高数据检索（SELECT查询）的速度**，以牺牲一定的写入（**INSERT**,** **UPDATE**, **DELETE**）性能和存储空间为代价。

  **如果没有索引，数据库在查询数据时必须执行**全表扫描（Full Table Scan）**，即逐行检查表中的每一条记录，直到找到匹配的行。当数据量巨大时，这种方式效率极低。**

 **索引通过对一个或多个列的值进行排序和组织，创建了一个可以快速定位到数据物理位置的“目录”。当查询条件涉及到这些列时，数据库可以直接通过索引快速找到对应数据的指针，从而避免全表扫描。**

#### 索引的底层数据结构

 **最常见的索引数据结构是** **B+树（B+ Tree）**：

* **平衡性**：B+树是一种自平衡的多路搜索树，无论数据如何增删，其从根节点到任意叶子节点的路径长度都保持一致，保证了查询性能的稳定。
* **高扇出（High Fan-out）**：B+树的非叶子节点只存储键值和指向下一层节点的指针，不存储数据。这使得单个节点可以容纳更多的键值，树的高度变得非常低。因为磁盘I/O是数据库性能的主要瓶颈，更低的树高意味着更少的磁盘读取次数。
* **有序性与范围查询**：所有的数据记录都存储在叶子节点上，并且这些叶子节点通过指针相互连接成一个有序的双向链表。这使得B+树不仅能高效地进行单点查询，还能非常高效地支持**范围查询（Range Queries）**。

 **除了B+树，还有**哈希索引（Hash Index）**，它通过哈希函数将键值映射到一个地址。哈希索引在**等值查询（Equality Queries）**场景下速度极快（O(1)），但不支持范围查询，并且在哈希冲突时性能会下降。**

#### 索引的优缺点

* **优点**：

  * **极大地加快数据检索速度。**
* **通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。**
* **缺点**：

  * **占用存储空间**：索引本身也是一张表，需要消耗磁盘空间。
* **降低写入性能**：当对表中的数据进行增、删、改时，索引也需要动态地维护，这会增加写入操作的耗时。

---

### 3. 怎么分库分表，怎么解决倾斜问题

#### 什么是分库分表（Sharding）？

  **当单一数据库的**数据量过大**（达到TB级别）或**并发请求量过高**（达到性能瓶颈）时，就需要进行分库分表。这是一种将数据分散到多个数据库或表中的技术，以提高系统的可伸缩性和性能。**

* **分库**：将一个数据库按业务或功能拆分成多个独立的数据库，部署在不同的服务器上。
* **分表**：将一个数据量巨大的单表，按照某种规则拆分成多个小表。

  * **垂直拆分（Vertical Sharding）**：按列拆分。将一个宽表（列很多）拆分成多个窄表，通常是把不常用的、或比较大的字段拆分出去。
* **水平拆分（Horizontal Sharding）**：按行拆分。将表中的数据行分散到多个结构相同的表中。这是最常见的分表方式。

#### 水平拆分的策略

 **关键在于选择一个**分片键（Shard Key）**，根据这个键的值来决定数据存放在哪个分片上。**

* **范围分片（Range-based Sharding）**：根据分片键的范围来划分。例如，用户ID 1-1,000,000 在分片1，1,000,001-2,000,000 在分片2。
* **优点：便于范围查询和扩容。**
* **缺点：容易产生**数据倾斜**和**热点问题**。例如，新注册的用户ID都集中在最后一个分片，导致该分片负载过高。**
* **哈希分片（Hash-based Sharding）**：根据分片键的哈希值来分配。例如，**shard_id = hash(user_id) % N**（N是分片总数）。

  * **优点：数据分布比较均匀，不容易产生热点。**
* **缺点：范围查询非常困难，需要查询所有分片；扩容（改变N的值）时需要进行大规模的数据迁移。**

#### 如何解决数据倾斜（Data Skew）问题？

**数据倾斜指的是因为分片键选择不当，导致数据和负载在不同分片上分布不均，某些分片成为系统的性能瓶颈。**

 **解决方法**：

* **选择合适的分片键**：这是预防倾斜的最好方法。分片键应具有高基数（唯一值多）和良好的随机性，能将数据均匀地散列到各个分片上。例如，使用用户的UUID而不是地区ID作为分片键。
* **热点数据二次分片**：如果某个分片键（例如一个明星用户的ID）本身就是热点，可以对这个热点数据进行再处理。

  * **二级哈希**：为热点键引入一个随机后缀或二级分片逻辑。例如，对于明星用户A的微博，分片规则可以变成 **shard_id = hash(user_A_id + post_id) % N**，这样他的微博就会被分散到多个分片中。
* **数据迁移和再平衡（Rebalancing）**：定期监控各个分片的负载和数据量。当检测到倾斜时，启动数据迁移程序，将热点分片上的部分数据迁移到负载较低的分片上。这个过程非常复杂，需要中间件或运维工具的支持。

---

### 4. 什么是模板元编程（Template Metaprogramming, TMP）

**模板元编程是一种**在编译期进行计算**的编程范式。它利用C++的模板机制，将本应在运行时执行的计算，提前到编译阶段来完成。**

 **简单来说，你写的代码不是直接的执行逻辑，而是**一套指令，用来指导编译器生成代码或在编译时计算出结果**。**

#### 核心思想

* **图灵完备**：C++模板系统是图灵完备的，意味着理论上它可以用来执行任何计算。
* **输入是类型，输出是类型或常量**：模板元程序的“变量”是类型，它的“逻辑”是通过模板特化和递归实例化来实现的。
* **编译期执行**：所有的计算都在编译时完成，最终生成的结果（常量或具体类型）被硬编码到可执行文件中。

#### 经典示例：编译期计算阶乘

**code**C++

```
// 递归的通用模板
template<int N>
struct Factorial {
    // 编译时常量，值为 N * (N-1)!
    static const int value = N * Factorial<N - 1>::value;
};

// 递归的终止条件：特化版本
template<>
struct Factorial<0> {
    static const int value = 1;
};

int main() {
    // Factorial<5>::value 在编译时就已经被计算出结果 120
    const int result = Factorial<5>::value; 
    // 上面这行代码和 const int result = 120; 等价
    return result;
}
```

#### 实际应用

* **性能优化**：将复杂的计算（如三角函数、复杂的常量表达式）在编译期完成，避免了运行时的开销。
* **静态检查和类型安全**：**std::static_assert** **可以在编译时检查条件是否满足。**std::type_traits**（如** **std::is_pointer**, **std::is_integral**）可以在编译期获取类型的各种属性，用于编写更安全、更通用的代码。
* **代码生成**：生成高度优化的代码，例如表达式模板（Expression Templates）技术，可以避免在复杂的数学运算（如矩阵运算）中产生临时对象。

---

### 5. 线程同步方式

**线程同步是指协调多个线程的执行顺序，以确保它们能够正确、有序地访问共享资源，避免数据竞争（Data Race）。**

**主要方式有：**

* **互斥锁（Mutex）**：最基本的同步机制。通过 **lock()** **和** **unlock()** **操作，确保同一时间只有一个线程能进入被保护的**临界区（Critical Section）**。适用于保护需要独占访问的资源。**
* **读写锁（Read-Write Lock）**：一种更高效的锁，它区分了读操作和写操作。

  * **允许多个线程同时进行读操作（“读-读”不互斥）。**
* **只允许一个线程进行写操作，且写操作与任何其他读/写操作都互斥（“写-写”、“读-写”互斥）。**
* **适用于**读多写少**的场景。**
* **信号量（Semaphore）**：一个计数器，用于控制能同时访问某个资源的线程数量。当计数值大于0时，线程可以访问资源并使计数减1；当计数值为0时，线程必须等待。它相当于一个广义的互斥锁（互斥锁是计数值为1的信号量）。
* **条件变量（Condition Variable）**：通常与互斥锁配合使用。它允许线程在某个条件不满足时，自动释放锁并进入等待状态。当其他线程满足了这个条件后，可以唤醒（**notify**）正在等待的线程。它避免了线程在循环中不断检查条件而空耗CPU的**忙等待（Busy Waiting）**问题。
* **原子操作（Atomic Operations）**：针对简单的整型或指针等数据类型，CPU提供了一些不可分割的原子指令（如 **fetch-add**, **compare-and-swap**）。C++通过 **std::atomic** **模板类提供了对这些操作的封装。对于简单的计数器或标志位操作，原子操作比使用锁的开销小得多，是一种高效的**无锁（Lock-Free）**同步方式。**

---

### 6. 线程通信方式

**线程通信是指在不同线程之间传递数据或信号。同步机制通常是实现可靠通信的基础。**

**主要方式有：**

* **共享内存（通过同步机制保护）**：这是最基本、最高效的通信方式。同一进程中的所有线程共享地址空间（堆、全局变量等），它们可以直接读写共享变量。

  * **关键**：必须使用互斥锁、原子操作等**同步机制**来保护对共享内存的访问，否则会导致数据竞争和不一致。
* **条件变量（Condition Variable）**：如上所述，它不仅是一种同步机制，更是一种高效的通信方式。一个线程可以通过它**等待**某个事件的发生，而另一个线程可以在事件发生后通过它**通知**等待的线程。
* **Promise / Future**：这是C++11引入的一种更现代的异步通信方式，专用于获取异步任务的**一次性结果**。

  * **Promise**：由任务的**生产者**（工作线程）持有，当任务完成后，生产者将结果放入Promise。
* **Future**：由任务的**消费者**（主线程或等待线程）持有，消费者可以通过Future的 **get()** **方法阻塞等待，直到结果被放入Promise。**
* **消息队列（Message Queue）**：一种常见的生产者-消费者模型实现方式。线程之间不直接交互，而是通过一个线程安全的队列来传递消息。

  * **一个或多个生产者线程向队列中放入消息。**
* **一个或多个消费者线程从队列中取出消息进行处理。**
* **队列本身是线程安全的，封装了底层的互斥锁和条件变量，使得上层代码更简洁。**
* **信号（Signal）**：在Linux/Unix系统中，信号是一种较为底层的通信方式，可以用于线程间的异步通知，但使用起来较为复杂且功能有限。在C++多线程编程中较少直接使用。
